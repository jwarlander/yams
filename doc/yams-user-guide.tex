\documentclass[a4paper,twoside,12pt]{article}

\usepackage{url}
\usepackage{listings}

\title{YAMS: Yet Another Monitoring System User Guide}
\author{Emma, Inc.}
\date{}

\begin{document}

\maketitle

\section{Introduction}

\textbf{YAMS} is a monitoring system.  It is composed of four main components:
\begin{enumerate}
  \item System Under Monitor (SUM)
  \item Extract, Load, Transform (ETL)
  \item Data Warehouse (DW)
  \item Web User Interface (WUI)
\end{enumerate}

Some words on expectations.

Take a modern Linux based operating system with 128 logical processors.  With
8 data points per logical processor (idle, interrupt, nice, softirq, steal,
system, user, and wait) and a sample interval of 60 seconds this results in
1,024 data points per sample interval.  The target for YAMS is many orders of
magnitude higher than this.

Now take a Postgres database with 500,000 tables.  Selecting all the columns
from \textit{pg\_stat\_all\_tables} and \textit{pg\_statio\_all\_tables}
results in 22 data points per table, or 11,000,000 data points per sample
interval.  The goal of YAMS is to be able to handle and analyze data sets of
this magnitude.

\section{Installation}

This section describes the software required by \textbf{YAMS}.

\begin{enumerate}
  \item PostgreSQL\footnote{\url{http://www.postgresql.org/}} needs to be
        installed onto the system to be used for the data
        warehouse.\footnote{The Postgres wiki has installation notes for
        various platforms:
        \url{http://wiki.postgresql.org/wiki/Detailed_installation_guides}} It
        is recommended that this system is a dedicated resource for the data
        warehouse.  \textbf{YAMS} was proofed against PostgreSQL 9.2.
  \item The \textbf{YAMS} ETL consists of three components.  There is a RESTful
        Web service, an ETL process in the \textbf{YAMS} \texttt{etl/}
        directory, and Redis.  The Web service is built as a FastCGI C program.
        The custom C FastCGI\footnote{\url{http://www.fastcgi.com/}} program
        processes the incoming HTTP POST data and pushes the data to Redis.
        Another program pops the data from Redis, transforms and loads the data
        in the data warehouse.  The ETL process is a C program that pops data
        out of Redis, processes it and loads the resulting data in to the data
        warehouse.  There is currently no official release of \textbf{YAMS}.
        The source code is available at \url{http://github.com/myemma/yams} via
        \texttt{git} or downloading a snapshot of the source.  It is
        recommended to install the \textbf{YAMS} ETL on a dedicated system.
        lighttpd and needs to installed the ETL system.  Redis is recommended
        to be installed on the ETL system but may be installed elsewhere.
  \item \textbf{collectd}\footnote{\url{http://www.collectd.org/}} is a system
        statistics collection daemon used on the systems under monitor.  At the
        time of this writing \textbf{YAMS} requires the use of a forked
        version\footnote{A set of changes has been submitted back to the
        \textbf{collectd} community:
        \url{http://mailman.verplant.org/pipermail/collectd/2011-January/004342.html}}
        of \textbf{collectd} 4.10.2 that is available via git or by downloading
        a snapshot of the source at
        \url{https://github.com/mwongatemma/collectd/tree/yams}.
        \textbf{collectd} generally needs to be installed onto the system under
        monitor.  The only exception covered in this documentation will be when
        using the \textit{postgresql} plugin.  Configuring, compiling, and
        installation instructions are in the \textbf{collectd} README file or
        online at
        \url{https://github.com/mwongatemma/collectd/blob/yams/README}.

\end{enumerate}

\section{Setup}

\subsection{PostgreSQL for the Data Warehouse}

Familiarity with how to administer and use PostgreSQL is helpful for this
section.

A PostgreSQL instance may need to be created depending on the operating system
used for the data warehouse.  If an instance needs to be created it is
recommended to use the database administrator operating system user, which is
typically \texttt{postgres}.  For example from the shell command line:
\lstset{language=sh}
\begin{lstlisting}
initdb /opt/pgdata
\end{lstlisting}

There are two PostgreSQL roles\footnote{A \textit{role} is also known as a
PostgreSQL database user}, \textit{collectd} and \textit{yams}, that are used
by \textbf{YAMS}.  The \textit{collectd} user is used by the \textbf{YAMS} ETL
to load data into the data warehouse.  This data is loaded into tables under
the \textit{collectd} schema.  The \textit{yams} user is used by the WUI to
query the data warehouse.  There is a \textit{yams} schema for tables
specifically used by the WUI.  These roles, schemas, tables and permissions can
be created by the database administrator operating system user using the
included shell script:
\lstset{language=sh}
\begin{lstlisting}
pg/create-database.sh
\end{lstlisting}

This script also attempts to use a tablespace named 'collectd'.  It is
currently expected that the PostgreSQL DBA create this tablespace in advance.
If this tablespace is not created then the default system tablespace will be
used.

The \texttt{pg\_hba.conf} file needs to be modified to allow connections from
the \textbf{YAMS} ETL system.

The \texttt{postgresql.conf} file needs to be modified to enable the TCP/IP
listener.  While it is against recommendations, if the ETL is run on the same
system as the data warehouse then the ETL can connect using a Unix-domain
socket.

\subsection{ETL}

The ETL has been tried with both lighttpd and nginx.  Lately nginx seems to be
more stable than running the ETL FastCGI app under lighttpd.

\subsubsection{Setting up nginx}

Familiarity with setting up nginx and FastCGI will help in this section.

The three environment variables REDIS\_SERVER, REDIS\_PORT, and REDIS\_KEY can
be set in the shell to customize the Redis connection.  If not specified the
default values will be set to what is shown in the example.

The FastCGI process needs to be started outside of nginx.  One way is to use
spawn-fcgi:
\lstset{language=sh}
\begin{lstlisting}
spawn-fcgi -p 9000 -n /usr/local/bin/yams-etl-fcgi
\end{lstlisting}

The portion of the nginx configuration to point to the FastCGI process is as
follows:
\lstset{language=clean}
\begin{lstlisting}
        location /yams/ {
            root   html;
            include        fastcgi_params;
            fastcgi_pass   127.0.0.1:9000;
        }
\end{lstlisting}

\subsubsection{Setting up lighttpd}

Familiarity with setting up lighttpd and FastCGI will help in this section.
This example only points out specific configuration parameters for
\textbf{YAMS}.  Specifically the HTTP port to listen to, how to activate the
FastCGI module, and how to route all requests to the C FastCGI service.  The
three environment variables REDIS\_SERVER, REDIS\_PORT, and REDIS\_KEY can be
set to specific Redis connection parameters.  If not specified the default
values will be set to what is shown in the example.  The \textit{bin-path}
parameters needs to be set to where the \textit{yams-etl-fcgi} program is
installed.

\lstset{language=clean}
\begin{lstlisting}
server.port = 8888

server.modules += ( "mod_fastcgi" )

fastcgi.server = ( "/" =>
                   ( "etl" =>
                     (
                       "socket" => socket_dir + "fastcgi.yams-etl.socket",
                       "bin-path" => "/usr/local/bin/yams-etl-fcgi",
                       "bin-environment" =>
                       (
                         "REDIS_SERVER" => "localhost",
                         "REDIS_PORT" => "6379",
                         "REDIS_KEY" => "yamsetl"
                       ),
                       "check-local" => "disable",
                       "max-procs" => 1,
                     )
                   )
                 )
\end{lstlisting}

\subsection{collectd}

The \textit{write\_http} output plugin must be enabled in order to send data to
the \textbf{YAMS} ETL in the \texttt{/opt/collectd/etc/collectd.conf} file:
\lstset{language=xml}
\begin{lstlisting}
LoadPlugin write_http
<Plugin write_http>
    <URL "http://etl_hostname:8888/">
        Format "JSON"
    </URL>
</Plugin>
\end{lstlisting}

None of the other output plugins are required for use with \textbf{YAMS} and
may be enabled as desired.  Any other plugin can be enabled.  See the
\textbf{collectd} documentation for details on the plugins
available.\footnote{\url{http://www.collectd.org/documentation.shtml}}  The
\textit{postgresql} plugin has special considerations detailed below.

\subsubsection{\textit{postgresql} plugin tips}

\textbf{YAMS} was specifically designed for monitoring PostgreSQL statistics.
The following examples show how to monitor database, table and index
statistics but continue to refer to the \textit{postgresql} plugin
documentation for an explanation of
parameters\footnote{\url{http://collectd.org/documentation/manpages/collectd.conf.5.shtml#plugin_postgresql}}.

Enable the \textit{postgresql} plugin:
\lstset{language=xml}
\begin{lstlisting}
LoadPlugin postgresql
\end{lstlisting}

With the modifications to the \textbf{collectd} \textit{postgresql} plugin we
can do slightly more meaningful queries.  More specifically it is now possible
to query the database, schema name, table name, and index name as part of the
query as meta data to the statistics retrieved.  For example the database
statistics can be collected for all databases in a PostgreSQL instance with a
single query using the following configuration:
\lstset{language=xml}
\begin{lstlisting}
TypesDB "/opt/collectd/etc/types.db.postgresql"

<Plugin postgresql>
    <Query stat_database>
        Statement "
SELECT datname, numbackends, xact_rollback, xact_commit, blks_read, blks_hit
FROM pg_stat_database;"

        <Result>
            Type database_stats
            InstancePrefix "database_stats"
            InstancesFrom "datname"
            ValuesFrom "numbackends" "xact_rollback" "xact_commit" "blks_read" "blks_hit"
        </Result>
    </Query>

    <Database some_pg_database>
        Host "some_pg_host"
        User "postgres"
        Query stat_database
        DatabasenameColumn 0
    </Database>
</Plugin>
\end{lstlisting}

A custom types.db file is needed to describe the data further.
\lstset{language=clean}
\begin{lstlisting}
database_stats numbackends:COUNTER:0:u, xact_rollback:COUNTER:0:u, xact_commit:COUNTER:0:u, blks_read:COUNTER:0:u, blks_hit:COUNTER:0:u,
\end{lstlisting}

Examples will be included in the source code distribution's \texttt{contrib/}
directory to show how to get table and index statistics.  Other examples will
show to collect statistics to determine table bloat, monitor transaction age
and vacuum efficiency.

\section{Get Running Quickly}

\begin{enumerate}
  \item Start the PostgreSQL database on the data warehouse system.  There may
        be init scripts available depending on the operation system installed
        on the data warehouse system.  Otherwise PostgreSQL can be started on
        the command line using the database administrator operating system
        account:
        \lstset{language=sh}
        \begin{lstlisting}
pg_ctl -D /opt/pgdata start
        \end{lstlisting}
  \item Start lighttpd:
  \item Start the \textbf{YAMS} ETL program:
        \lstset{language=sh}
        \begin{lstlisting}
yams-etl --pghost dw_hostname
        \end{lstlisting}
  \item Start the \textbf{collectd} daemon:
        \lstset{language=sh}
        \begin{lstlisting}
/opt/collectd/sbin/collectd
        \end{lstlisting}
\end{enumerate}

\section{Components}

This section describes each component in more detail than the previous section.

\subsection{PostgreSQL}

The database schema design takes advantage of table inheritance to physically
partition data.  This is how the parent table is defined:
\lstset{language=sql}
\begin{lstlisting}
CREATE TABLE value_list (
    time TIMESTAMP WITH TIME ZONE NOT NULL,
    interval INTEGER NOT NULL,
    host VARCHAR(64) NOT NULL,
    plugin VARCHAR(64) NOT NULL,
    plugin_instance VARCHAR(64),
    type VARCHAR(64) NOT NULL,
    type_instance VARCHAR(64),
    dsnames VARCHAR(512)[] NOT NULL,
    dstypes VARCHAR(8)[] NOT NULL,
    values NUMERIC[] NOT NULL
);
\end{lstlisting}

All timestamps are stored in the database in UTC.

Each \textbf{collectd} plugin partitions the \textit{value\_list} table by day.
The \textit{postgresql} plugin take things further because of the potentially
high volume of data that can be produced by a PostgreSQL database under
monitor.  The \textit{postgresql} plugin inherits the \texttt{value\_list} and
augments it as:

\lstset{language=sql}
\begin{lstlisting}
CREATE TABLE vl_postgresql (
    database VARCHAR(64) NOT NULL,
    schemaname VARCHAR(64),
    tablename VARCHAR(64),
    indexname VARCHAR(64),
    metric VARCHAR(64) NOT NULL,
    CHECK (plugin = 'postgresql')
) INHERITS (value_list);
\end{lstlisting}

The \textit{postgresql} plugin creates partition tables based on metric per
day.  Using PostgreSQL terminology the \texttt{database}, \texttt{schemaname},
\texttt{tablename} and \texttt{indexename} are self-explanatory.  The
\texttt{metric} column is the name of the particular statistic being collected.
For example, the seq\_scan column from the pg\_stat\_all\_tables table would
set the \texttt{metric} column to `seq\_scan'.

\subsection{ETL}

The \textbf{YAMS} ETL is a custom RESTful Web service.  It uses a lighttpd to
listen for HTTP POST requests from \textbf{collectd} and queues up the incoming
JSON data by using a C FastCGI progream to pushi it to Redis.  Another program
pops the data out of Redis and processes the data by transforming the JSON
objects into SQL INSERT statements and loads the data into the data warehouse.

The database partitioning is also handled in the \textbf{YAMS} ETL.

\end{document}
